{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1909.03186.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 「聞いた人がその分野について何らかの傾向をつかめる発表\n",
    "にする。」 \n",
    "- 今回は研究論文の詳細まで伝える必要はない。関連する論文\n",
    "の傾向を見つけ出し、それを伝えられるようにする。\n",
    "- 「論文の選定理由を答えられるようにする。」\n",
    "- 自分の問題意識や、受けたい企業の課題に結びついた技術に\n",
    "関する論文である、論文の引用数が多い、など。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# どんなものか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 長い論文の要約生成\n",
    "- ようやく生成前に１ステップを踏むことで精度を上げた\n",
    "- 重要な文の抽出→要約の生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# どうやって有効か検証した？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- F-1 ROUGEスコア "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 技術の手法と肝は"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- エンコーダーデコーダーアーキテクチャーを使用して抽象的要約をseq2seq問題として定式化する代わりに、適切に「フォーマットされた」データを使用してゼロからトレーニングされた単一のトランスフォーマー言語モデルのみを使用します\n",
    "- 抽出ステップと抽象ステップ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 議論はあるか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 先行研究と比べて何がすごい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- トランスフォーマー言語モデルは、長い科学記事の要約に驚くほど効果的であり、コピーメカニズムがなくても、典型的なseq2seqアプローチより優れていることを実証します。\n",
    "\n",
    "- 私たちのアプローチは、より高いROUGEスコアを達成しながら、コピーメカニズム[ 29 ]を使用する従来の作業と比較して、より多くの「抽象的」要約を生成することを示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 次に読むべき論文は"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-e0e9b00cfec6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-e0e9b00cfec6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    http://higepon.hatenablog.com/entry/20171210/1512887715\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "http://higepon.hatenablog.com/entry/20171210/1512887715\n",
    "http://kysmo.hatenablog.jp/entry/2017/10/24/105421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 要約を生成する前に簡単な抽出ステップを実行し、それを使用して、関連情報に基づいてトランスフォーマー言語モデルを条件付けしてから、要約の生成\n",
    "- ROUGE-NのNはN-gramのNで、生成した要約と人間が作成した要約がN-gram単位でどれだけ一致しているかを計測した指標です。ROUGE-1なら単語単位、ROUGE-2ならbi-gram単位の一致になります。例えば、\"いちご ばなな\"と\"ばなな いちご\"は、ROUGE-1単位(単語単位)では一致しますが、ROUGE-2、つまりbi-gramなら\"いちご ばなな\"が一単位になるため一致はしなくなります。これは即ち正解の要約文中にあるN-gramの組み合わせがどれだけ生成した要約に出てくるかを表しており、乱暴に言えば要約文をすごく長くすればそれだけROUGEスコアは高くなる傾向があります。そうした意味で、ROUGEによる評価はRecall的といえます。もちろん、これだけでは「実際の要約にはない」単語=ミスが評価されません。そこで出てくるのが、次のBLEUスコアです。\n",
    "-　BLEUは、生成した要約のN-gram中のどれだけが実際の要約に登場するかで評価されます(なお、この場合長い文は短い文に比べてN-gram数が大きくなり不利になるため、調整が行われています)。BLEUは分母に生成した要約のN-gram数が来るため、乱発すればそれだけ一致する率は下がることになります。このため、Precision的な評価が可能になります。なお、BLEUは翻訳のタスクでよく使用されています。\n",
    "\n",
    "実際には、評価ではROUGEが使われることが多いです。というのも、たいていの場合要約の長さには制限があり、「長々生成してスコアを稼ぐ」という戦略自体が利用できないためです。\n",
    "\n",
    "\n",
    "-　コピーメカニズムを使用する従来の作業\n",
    "-\n",
    "-　Transformerの言語モデルは、抽出ステップとそれに続く抽象ステップを介して、テキストの長いシーケンスの高品質な要約を生成できることを実証しました。\n",
    "-\n",
    "-\n",
    "- N-gramとは\n",
    "- コトバンクでは以下の様になってました。\n",
    "任意の文字列や文書を連続したn個の文字で分割するテキスト分割方法．特に，nが1の場合をユニグラム（uni-gram），2の場合をバイグラム（bi-gram），3の場合をトライグラム（tri-gram）と呼ぶ．\n",
    "要するにテキストの分割方法なんですが、いまいちピンと来ない人もいるかと思います。\n",
    "\n",
    "\n",
    "-　エンコードとは一般にデータをある規則に従い、変換すること、デコードはエンコードした情報を元に戻すことをいいます。なので、エンコーダー、デコーダーはどの文脈でその言葉が出てきたのかによって意味がかわってきます。\n",
    "\n",
    "-　Attention\n",
    "そもそもattentionとは、入力の特徴量のなかで着目すべき部分とそれに対応する知識を同時に学習しようというものでした。\n",
    "私たちが言語を理解するとき、その入力クエリ（例えば読んだり聞いたりした文章）の一語一句全てをちゃんと理解している訳ではなく、ある特定の単語からそれに関する情報を想起し、関係を把握し、解釈することになります。attentionではそうした構造をモデル化して（いると私は解釈している）います。\n",
    "端的に言うとattentionは、入力ベクトルによって重みを計算し、その重みに基づいてメモリから任意のベクトルを出力するモデルです。\n",
    "ここで登場する変数は３つ。\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
